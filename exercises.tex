% Notes and exercises from Foundations of Modern Analysis by Dieudonne
% By John Peloquin
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier}

\newcommand{\N}{\mathbf{N}}
\newcommand{\Nex}{\overline{\N}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\Rp}{\R_+}
\newcommand{\Rps}{\Rp^*}
\newcommand{\Rex}{\overline{\R}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\Re}{\mathcal{R}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\BC}{\mathcal{C}^{\infty}}
\renewcommand{\L}{\mathcal{L}}

\newcommand{\union}{\cup}
\newcommand{\sect}{\cap}
\newcommand{\after}{\circ}
\newcommand{\grad}{\nabla}
\newcommand{\at}{\cdot}
\newcommand{\downto}{\downarrow}
\newcommand{\iso}{\cong}

\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\closure}[1]{\overline{#1}}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\abs}[1]{|{#1}|}
\newcommand{\norm}[1]{\lVert{#1}\rVert}
\newcommand{\bignorm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\innerprod}[2]{({#1}|{#2})}
\newcommand{\cbprod}[2]{[{#1}\cdot{#2}]}
\renewcommand{\d}[1]{\,d\!{#1}}
\newcommand{\dx}{\d{x}}
\newcommand{\dzeta}{\,d\zeta}

% Theorems
\theoremstyle{plain}
\newtheorem*{prop}{}

\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

% Meta
\title{Notes and exercises from\\\textit{Foundations of Modern Analysis}}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle

\section*{Introduction}
This document contains notes and exercises from~\cite{dieudonne}.

\medskip
\noindent Some notable features of the book:
\begin{itemize}[itemsep=0pt]
\item Among the axioms of the real numbers, the archimedean property and the nested interval property are included; they are used to prove the least upper bound property~(2.3.2) and completeness~(3.14.3).
\item Topological notions are introduced in the context of metric spaces, and then specialized to the real line.
\item Continuity is defined topologically. Functional limits are defined in terms of continuity, with sequential limits as a special case. Where other books might work directly with sequences in proofs, Dieudonn\'e often works with composites of continuous functions, applying general theorems like extension principles (3.15.2) and~(3.15.4). (The difference in style is like that between functional programming and imperative programming in computer science.)
\item The real logarithm function is introduced as a unique continuous homomorphism from the multiplicative group of positive reals to the additive group of reals~(4.3.2). The real exponential function is then defined as the inverse of the real logarithm function.
\item In the proof of the Tietze-Urysohn extension theorem~(4.5.1), an explicit formula for the extension is given.
\item Series are introduced in the context of normed linear spaces, with a focus on absolutely convergent series in Banach spaces, providing for a unified treatment of series of numbers and functions.
\item The derivative is defined for continuous functions between Banach spaces, and then specialized to functions between Euclidean spaces.
\item The Cauchy integral (of ``regulated'' functions) is studied instead of the Riemann integral.
%\item The Jordan curve theorem~(Ap.4.2) is proved.
\end{itemize}

\section*{Chapter~III}
\subsection*{Section~14}
\begin{rmk}
If \(f\)~is a mapping from a metric space~\(E\) into a metric space~\(E'\), then a necessary and sufficient condition for \(f\)~to be continuous at \(x\in E\) is that the oscillation of~\(f\) at~\(x\), with respect to~\(E\), be~\(0\).
\end{rmk}
\begin{proof}
By definition and~(3.11.2).
\end{proof}

\section*{Chapter~IV}
\subsection*{Section~3}
\begin{rmk}
In the proof of~(4.3.1), intuitively we know that \(f=\log_a\). Recall by the density of the rationals in the reals (2.2.16) that \emph{a real number is the supremum of the set of rationals less than it}. Therefore \(\log_a x\)~is just the supremum of the set of rationals~\(m/n\) with \(m/n\le\log_a x\), or equivalently \(a^{m/n}\le x\), or equivalently \(a^m\le x^n\); that is, \(\log_a x=\sup A_x\). Since neither \(\log_a x\) nor~\(a^{m/n}\) are defined yet, but \(a^m\)~and~\(x^n\) are, it is natural to show that \(f(x)=\sup A_x\) for uniqueness.

For existence, we know the set~\(A_x\) is well defined. Fix \(k\ge 1\) and choose~\(p,q\) by~(4.3.1.1) with \(a^p\le x^k\le a^q\). Then \(p/k\in A_x\), so \(A_x\)~is nonempty. If \(m/n\in A_x\) (\(n\ge 1\)), then \(a^m\le x^n\), so \(a^{mk}\le x^{nk}\le a^{qn}\), so \(mk\le qn\), and \(m/n\le q/k\); that is, \(A_x\)~is bounded above by~\(q/k\). Therefore \(f(x)=\sup A_x\)~is defined, and \(p/k\le f(x)\le q/k\). Taking \(q=p+1\) yields \(p/k\le f(x)\le(p+1)/k\), which is used in the proof that \(f(xy)=f(x)+f(y)\). Taking \(x=a\) and \(k=p=q=1\) yields \(f(a)=1\).


\end{rmk}

\noindent We provide an alternative proof of~(4.3.7):
\begin{prop}[4.3.7]
Any continuous mapping~\(g\) of~~\(\Rps\) into itself such that \(g(xy)=g(x)g(y)\) has the form \(x\mapsto x^a\), with \(a\in\R\).
\end{prop}
\begin{proof}
If \(b>1\), then \(f=\log_b\after g:\Rps\to\R\) is continuous and
\[f(xy)=\log_b g(xy)=\log_b[g(x)g(y)]=\log_b g(x)+\log_b g(y)=f(x)+f(y)\]
If \(g\ne1\), there is \(x>0\) with \(g(x)>1\) (if \(g(x)<1\), then \(g(\inv{x})=\inv{g(x)}>1\)), so \(y_n=g(x^n)=g(x)^n\to\infty\) as \(n\to\infty\) and \(y_n\to 0\) as \(n\to-\infty\). It follows from (3.19.1) and~(3.19.7) that \(g\)~is surjective, so there is \(c>0\) with \(c\ne 1\) such that \(g(c)=b\) and hence \(f(c)=1\). By~(4.3.2), \(f=\log_c\), so
\[g(x)=b^{\log_c x}=b^{a\log_b x}=x^a\]
where \(a=\log_c b\), by (4.3.3) and~(4.3.4).
\end{proof}
\begin{rmk}
This proof is longer than Dieudonn\'e's, but avoids direct use of~(4.1.3), which is already used in the proof of~(4.3.2).
\end{rmk}

\subsection*{Section~5}
\begin{rmk}
In the proof of the Tietze-Urysohn extension theorem~(4.5.1), the idea behind the formula defining~\(g(x)\) for \(x\in E-A\) is roughly: \emph{for \(x\)~very near the boundary (frontier) of~\(A\), take the limit of~\(f(y)\) as \(y\in A\) approaches points near~\(x\)}:
\[g(x)=\inf_{y\in A}\left[f(y)\frac{d(x,y)}{d(x,A)}\right]\qquad(x\in E-A)\]
Write \(\rho_x(y)=d(x,y)/d(x,A)\). Since \(1\le f(y)\le 2\) by the reduction, \(\rho_x(y)\ge 1\), \(\rho_x(y)>2\) for \(y\)~sufficiently far from~\(x\), and \(\rho_x(y)\to 1\) as \(y\to x\), it follows that \(\inf_{y\in A}f(y)\rho_x(y)\) reflects values of~\(f(y)\) at \(y\)~nearest to~\(x\). This ensures continuity of~\(g\) on the boundary of~\(A\). Since \(g\)~is also continuous in the interior of~\(A\) (by continuity of~\(f\)), and in the exterior of~\(A\) (by continuity of the metric~\(d\)), \(g\)~is continuous on~\(E\).
\end{rmk}

\section*{Chapter~V}
\subsection*{Section~2}
\begin{rmk}
(5.2.4)~is an associativity property for convergent series in a normed space. For \emph{absolutely} convergent series in a \emph{Banach} space, (5.3.6)~is a stronger associativity property. For example, if \(\sum_{n=0}^{\infty}x_n\) is absolutely convergent in~\(\C\), it follows from the remark below~(5.3.6), but not from~(5.2.4), that \(\sum_{n=0}^{\infty}x_n=\sum_{n=0}^{\infty}x_{2n}+\sum_{n=0}^{\infty}x_{2n+1}\).
\end{rmk}

\subsection*{Section~3}
\begin{rmk}
In the proof of~(5.3.2), to see how the inequality \(\norm{\sum_{n=0}^{\infty}x_n}\le\sum_{n=0}^{\infty}\norm{x_n}\) follows from the principle of extension of inequalities~(3.15.4), recall from the definition of sequential limit in~(3.13) that the functions given by
\[s(n)=\sum_{k=0}^n x_k\quad s(\infty)=\sum_{k=0}^{\infty}x_k\qquad\text{and}\qquad t(n)=\sum_{k=0}^n\norm{x_k}\quad t(\infty)=\sum_{k=0}^{\infty}\norm{x_k}\]
are \emph{continuous} on \(\N\union\{\infty\}=\Nex\subseteq\Rex\). By continuity of the norm, the composite function \(n\mapsto\norm{s(n)}\) is also continuous on~\(\Nex\). Now \(\norm{s(n)}\le t(n)\) for all \(n\in\N\) and \(\N\)~is dense in~\(\Nex\), hence \(\norm{s(\infty)}\le t(\infty)\) by~(3.15.4), as desired. Other applications of the principle of extension are similar.
\end{rmk}

\begin{rmk}
In~(5.3.4), \(\sum_{\alpha\in A}\norm{x_{\alpha}}=\sup\left\{\,\sum_{\alpha\in J}\norm{x_{\alpha}}\mid J\subseteq A\text{ finite}\,\right\}\) by~(5.3.1). This is implicit in the proof of~(5.3.5).
\end{rmk}

\begin{rmk}
In~(5.3.4), \(2\epsilon\)~can be replaced by~\(\epsilon\), since \(H\)~can be chosen for~\(\epsilon/2\).
\end{rmk}

\begin{rmk}
In the proof of~(5.3.4), to construct~\(H\), first fix a bijection \(\varphi:\N\to A\). Then \(\sum_{n=0}^{\infty}\norm{x_{\varphi(n)}}\) converges, so there exists~\(M\) such that for all \(n\ge m\ge M\), \(\sum_{k=m}^n\norm{x_{\varphi(k)}}\le\epsilon\) (5.2.1). Let \(H=\varphi[0,M)\). Then if \(K\subseteq A\) is finite with \(H\sect K=\emptyset\), \(\inv{\varphi}[K]\subseteq[M,\infty)\), so if \(N=\max\inv{\varphi}[K]\), then \(\sum_{\alpha\in K}\norm{x_{\alpha}}\le\sum_{k=M}^N\norm{x_k}\le\epsilon\). Also \(\norm{\sum_{\alpha\in A}x_{\alpha}-\sum_{\alpha\in H}x_{\alpha}}=\norm{\sum_{n=M}^{\infty}x_{\varphi(n)}}\le\sum_{n=M}^{\infty}\norm{x_{\varphi(n)}}\le\epsilon\) by (5.3.1)~and~(5.3.2). Finally, if \(L\)~is finite with \(H\subseteq L\subseteq A\), then \(L-H\)~is finite with \((L-H)\sect H=\emptyset\), so \(\norm{\sum_{\alpha\in H}x_{\alpha}-\sum_{\alpha\in L}x_{\alpha}}=\norm{\sum_{\alpha\in L-H}x_{\alpha}}\le\sum_{\alpha\in L-H}\norm{x_{\alpha}}\le\epsilon\), and it follows that \(\norm{\sum_{\alpha\in A}x_{\alpha}-\sum_{\alpha\in L}x_{\alpha}}\le 2\epsilon\).
\end{rmk}

\begin{rmk}
In the proof of~(5.3.6), the idea is to use finite approximations of finitely many of the~\(z_n\) to obtain a finite approximation of~\(\sum_{\alpha\in A}x_\alpha\).
\end{rmk}

\subsection*{Section~5}
\begin{rmk}
In a Banach space, \(\lambda\sum_{n=0}^{\infty}x_n=\sum_{n=0}^{\infty}\lambda x_n\) for any scalar~\(\lambda\), when either series converges, by (5.1.5)~and~(5.5.2).
\end{rmk}

\begin{rmk}
Let \(L_1(I)\)~be the space of Lebesgue integrable real-valued functions\footnote{Technically this space consists of equivalence classes of functions.} on the interval \(I\subseteq\R\) with norm \(\norm{f}_1=\int_I\abs{f}\). Then \(L_1(I)\)~is a Banach space (Riesz-Fischer). The integral operator \(f\mapsto\int_If\) is linear and continuous since \(\abs{\int_If}\le\int_I\abs{f}\) (5.5.1). Therefore if \(f=\sum_{n=0}^{\infty}f_n\) under convergence in the norm, then \(\int_I f=\sum_{n=0}^{\infty}\int_I f_n\) (5.5.2).
\end{rmk}

\begin{rmk}
The multiplication operation \((x,y)\mapsto xy\) from~\(\C^2\) to~\(\C\) is bilinear and continuous by~(4.4.1), hence if \(\sum_{n=0}^{\infty}x_n\) and \(\sum_{n=0}^{\infty}y_n\) are absolutely convergent series of complex numbers, then \((\sum_{n=0}^{\infty}x_n)(\sum_{n=0}^{\infty}y_n)=\sum_{m,n}x_my_n\), where the latter sum may be taken \emph{in any order}, by~(5.5.3).
\end{rmk}

\begin{rmk}
In the proof of~(5.5.4), to see how the linearity of~\(\closure{f}\) follows from the principle of extension of identities~(3.15.2), observe that the mappings
\[(x,y)\mapsto x+y\mapsto\closure{f}(x+y)\qquad\text{and}\qquad(x,y)\mapsto(\closure{f}(x),\closure{f}(y))\mapsto\closure{f}(x)+\closure{f}(y)\]
are continuous by (3.11.5), (3.20.15), and~(5.1.5), and agree on~\(G^2\), which is dense in~\(E^2\) by~(3.20.3), and hence agree on~\(E^2\) by~(3.15.2). Other applications of the principle of extension are similar.
\end{rmk}

\subsection*{Section~8}
\begin{rmk}
Intuitively, a \emph{closed} hyperplane separates a vector space into the two half-spaces on either side of the hyperplane, but a \emph{dense} hyperplane does not have this property.
\end{rmk}

\section*{Chapter~VI}
\subsection*{Section~3}
\begin{rmk}
(6.3.1.1)~is the \emph{parallelogram law}, which states that the sum of the squares of the lengths of the diagonals of a parallelogram is equal to the sum of the squares of the lengths of its sides.
\end{rmk}

\begin{rmk}
In the proof of~(6.3.1),
\begin{align*}
\norm{x-(y+\lambda z)}^2&=\innerprod{x-y-\lambda z}{x-y-\lambda z}\\
	&=\norm{x-y}^2-\lambda[\innerprod{x-y}{z}+\innerprod{z}{x-y}]+\lambda^2\norm{z}^2\\
	&=\alpha^2-2\lambda\Re\innerprod{x-y}{z}+\lambda^2\norm{z}^2>\alpha^2
\end{align*}
so
\[-2\lambda\Re\innerprod{x-y}{z}+\lambda^2\norm{z}^2>0\tag{1}\]
Substituting~\(-\lambda\) for~\(\lambda\) in~(1) yields
\[2\lambda\Re\innerprod{x-y}{z}+\lambda^2\norm{z}^2>0\tag{2}\]
Now (1)~and~(2) imply
\[\abs{\Re\innerprod{x-y}{z}}<\lambda\frac{\norm{z}^2}{2}\qquad(\lambda>0)\]
Letting \(\lambda\downto 0\) shows \(\Re\innerprod{x-y}{z}=0\).
\end{rmk}

\subsection*{Section~5}
\begin{rmk}
If \(E\)~is the Hilbert sum of a normal system~\((a_n)\) and \(x=(x_n)\in E\), then \(x_n=\lambda_n a_n\) with
\[\innerprod{x}{j_n(a_n)}=\sum_{m=1}^{\infty}\innerprod{\lambda_ma_m}{\delta_{mn}a_m}=\lambda_n\norm{a_n}^2=\lambda_n\]
It follows that \(x=\sum_{n=1}^{\infty}\innerprod{x}{j_n(a_n)}j_n(a_n)\) and this representation in~\((j_n(a_n))\) is unique. If \(y\in E\),
\[\innerprod{x}{y}=\sum_{n=1}^{\infty}\innerprod{x}{j_n(a_n)}\conj{\innerprod{y}{j_n(a_n)}}\]
In particular \(\norm{x}^2=\sum_{n=1}^{\infty}\abs{\innerprod{x}{j_n(a_n)}}^2\).
\end{rmk}

\begin{rmk}
In the proof of~(6.5.2), to see how the result follows from~(6.4.2), let \(\varphi\)~be the isomorphism of~\(F\) onto the Hilbert sum~\(E\) of the~\(a_n\) with \(\varphi(a_n)=j_n(a_n)\). If \(x\in F\), by the previous remark and properties of~\(\varphi\),
\[\varphi(x)=\sum_{n=1}^{\infty}\innerprod{\varphi(x)}{\varphi(a_n)}\varphi(a_n)=\varphi\left[\sum_{n=1}^{\infty}\innerprod{x}{a_n}a_n\right]\]
so \(x=\sum_{n=1}^{\infty}\innerprod{x}{a_n}a_n\). If \(y\in F\), then
\[\innerprod{x}{y}=\innerprod{\varphi(x)}{\varphi(y)}=\sum_{n=1}^{\infty}\innerprod{\varphi(x)}{\varphi(a_n)}\conj{\innerprod{\varphi(y)}{\varphi(a_n)}}=\sum_{n=1}^{\infty}\innerprod{x}{a_n}\conj{\innerprod{y}{a_n}}\]
In particular \(\norm{x}^2=\sum_{n=1}^{\infty}\abs{\innerprod{x}{a_n}}^2\). If \((\lambda_n)\)~is a sequence of scalars with \(\sum_{n=1}^{\infty}\abs{\lambda_n}^2\) convergent, then \(z=(\lambda_na_n)\in E\) and \(x=\inv{\varphi}(z)\) is unique such that \(\innerprod{x}{a_n}=\innerprod{z}{\varphi(a_n)}=\lambda_n\).
\end{rmk}

\section*{Chapter~VII}
\subsection*{Section~1}
\begin{rmk}
In~(7.1.2.1), the mappings~\(f_i\) are the \emph{scalar component functions} of~\(f\), and the mappings \(t\mapsto f_i(t)a_i\) are the \emph{vector component functions} of~\(f\).

(7.1.2)~shows that if \(F\)~is finite-dimensional, then \(\B_F(A)\)~decomposes into subspaces of vector component functions, each of which is isomorphic to a space of scalar component functions. By~(5.9.1), this result can be expressed in the equivalent form
\[\B_{K^n}(A)\iso\B_K(A)^n\]
where \(K=\R\) or \(K=\C\).
\end{rmk}

\subsection*{Section~2}
\begin{rmk}
If \(E\)~is a metric space and \(K=\R\) or \(K=\C\), then \(\BC_{K^n}(E)\iso\BC_K(E)^n\).
\end{rmk}

\begin{rmk}
If \(E\)~is a metric space and \(F\)~is a Banach space, then \(\BC_F(E)\)~is a Banach space, by (7.2.1), (7.1.3), and~(3.14.5).
\end{rmk}

\begin{rmk}
The proof of Dini's theorem~(7.2.2) is like the proof of~(7.2.1) ``turned on its side.''
\end{rmk}

\subsection*{Section~5}
\begin{rmk}
(7.5.6) is like Dini's theorem~(7.2.2), but with equicontinuity replacing monotonicity and continuity.
\end{rmk}

\subsection*{Section~6}
\begin{rmk}
If \(f:I\to F\) is regulated and \(g:F\to G\) is continuous, then \(g\after f:I\to G\) is regulated, by~(3.13.6). In particular, \(\xi\mapsto\norm{f(\xi)}\)~is regulated.
\end{rmk}

\begin{rmk}
If \(f:I\to\R\) is monotone and \(g:J\to F\) is regulated with \(f(I)\subseteq J\), then \(g\after f:I\to F\) is regulated, by (4.2.1) and~(3.13.6).
\end{rmk}

\begin{rmk}
If \(f_i:I\to F_i\) is regulated for all \(1\le i\le n\), then \(f=(f_i):I\to\prod F_i\) is regulated, by~(3.20.5). It follows that \(\sum f_i\)~is regulated, by (5.1.5)~and the first remark above.
\end{rmk}

\begin{rmk}
The uniform limit of a sequence of regulated functions on a compact interval is regulated, by~(7.6.1).
\end{rmk}

\section*{Chapter~VIII}
\subsection*{Section~1}
\begin{rmk}
It is possible to extend the definition of differentiability, and most of the results in this chapter, to functions between arbitrary \emph{normed} spaces.
\end{rmk}

\begin{rmk}
It is possible instead to have continuity of derivatives by definition, and continuity of differentiable functions by theorem.
\end{rmk}

\begin{rmk}
In the proof of~(8.1.4), note that \(\norm{s}\le\epsilon/c\) and \(\norm{t}\le\norm{(s,t)}\), so \(\norm{\cbprod{s}{t}}\le c(\epsilon/c)\norm{(s,t)}=\epsilon\norm{(s,t)}\).
\end{rmk}

\begin{rmk}
In~(8.1.5), the identification \(\L(E;\prod F_i)\iso\prod\L(E;F_i)\) is like~(7.1.2).
\end{rmk}

\subsection*{Section~2}
\begin{rmk}
(8.2.3)~is a weak form of the inverse function theorem~(10.2.5). Under appropriate conditions, it establishes \emph{differentiability} of an inverse function, \emph{assuming existence of the inverse function}, while the inverse function theorem establishes local \emph{existence and differentiability} of an inverse function.

In the proof, the change in~\(g\) at~\(y_0\) is inverse to the change in~\(f\) at~\(x_0\). Since the latter is well approximated by~\(f'(x_0)\) in a sufficiently small neighborhood, it is natural that the former is well approximated by~\(\inv{f'(x_0)}\). More specifically, if \(z=g(y_0+t)-g(y_0)\), then \(t=f(x_0+z)-f(x_0)\approx f'(x_0)\at z\), so \(\inv{f'(x_0)}\at t\approx z\).
\end{rmk}

\subsection*{Section~4}
\begin{rmk}
If \(E\)~and~\(F\) are one-dimensional (identified to \(\R\)~or~\(\C\)) and \(f:E\to F\) is differentiable, then
\[(fg)'=f'g+fg'\]
by (8.1.4), (8.1.5), and~(8.2.1). This is just the \emph{product rule}.
\end{rmk}

\begin{rmk}
The notation~\(f'_d(\alpha)\) (resp.~\(f'_g(\beta)\)) for derivative on the right (resp. left) at the point~\(\alpha\) (resp.~\(\beta\)) is inspired by the French word ``droite'' (resp. ``gauche'') for right (resp. left).
\end{rmk}

\subsection*{Section~5}
\begin{rmk}
In the proof of the mean value theorem~(8.5.1), the idea is to prove arbitrarily close approximations to the desired inequality by ``induction'' on subintervals~\([\alpha,\gamma]\) of~\([\alpha,\beta]\).
\end{rmk}

\begin{rmk}
Results about differentiable functions defined on intervals in~\(\R\) can be extended to functions defined on (neighborhoods of) segments in Banach spaces using the chain rule, as seen in the proof of~(8.5.4) and also the proof of Taylor's theorem~(8.14.3).
\end{rmk}

\subsection*{Section~6}
\begin{rmk}
If a function~\(f\) is differentiable at a point~\(x_0\), then change in~\(f\) \emph{at~\(x_0\)} is well approximated by~\(f'(x_0)\). If \(f\)~is \emph{continuously} differentiable at~\(x_0\), then change in~\(f\) \emph{between any two points sufficiently close to~\(x_0\)} is well approximated by~\(f'(x_0)\), by (8.6.2).\footnote{See also Problem~3.}
\end{rmk}

\subsection*{Section~7}
\begin{rmk}
By~(7.6.1), a regulated function \(f:I\to\R\), where \(I\subseteq\R\) is a compact interval, is uniformly approximated by real-valued step functions, and hence intuitively has a well defined ``area under its curve'', namely the limit of the areas under its approximating step functions.

On the other hand, in the proof of~(8.7.2), we see that a primitive of a step function gives the area under its curve, and since the primitive of~\(f\) is just the limit of the primitives of its approximating step functions (8.6.4), the primitive of~\(f\) gives the area under~\(f\). This motivates the definition of the integral in terms of primitives (the Cauchy integral).
\end{rmk}

\begin{rmk}
The Cauchy integral makes one half of the fundamental theorem of calculus true by definition; the other half is~(8.7.3).
\end{rmk}

\begin{rmk}
In the proof of~(8.7.3), to see how the inequality follows from~(8.5.2), let \(h(x)=g(x)-f(\xi)x\). Since \(g\)~is a primitive of~\(f\), there is a denumerable set \(D\subseteq I\) such that for all \(x\in I-D\), \(h'(x)=f(x)-f(\xi)\). It follows from~(8.5.2) that for any \(0\le\zeta\le\lambda\),
\[\norm{h(\xi+\zeta)-h(\xi)}\le\zeta\sup_{0\le\eta\le\lambda}\norm{f(\xi+\eta)-f(\xi)}\]
as desired. This is like the proof of~(8.6.2) from~(8.5.4).
\end{rmk}

\begin{rmk}
In the proof of~(8.7.4), to see that \(\xi\mapsto f(\varphi(\xi))\varphi'(\xi)\) is regulated, first note that if \(f\)~is continuous, then \(f\after\varphi\)~is continuous by~(3.11.5), and hence regulated, by~(7.6.2); on the other hand if \(\varphi\)~is monotone, then \(f\after\varphi\)~is regulated, by a remark from~(7.6) above. Since \(\varphi'\)~is also regulated,
\[\xi\mapsto(f(\varphi(\xi)),\varphi'(\xi))\mapsto f(\varphi(\xi))\varphi'(\xi)\]
is regulated, by~(5.1.5) and remarks from~(7.6) above.
\end{rmk}

\begin{rmk}
In the proof of~(8.7.5), note for example
\[\xi\mapsto(f(\xi),g'(\xi))\mapsto\cbprod{f(\xi)}{g'(\xi)}\]
is regulated, by (7.6.2)~and remarks from~(7.6) above. Also note use of additivity of the integral in the proof, which follows from~(8.2.2).
\end{rmk}

\begin{rmk}
In the proof of~(8.7.6), \(u\after f\)~is regulated, by a remark from~(7.6) above.
\end{rmk}

\begin{rmk}
In the proof of the mean value theorem for integrals~(8.7.7), \(\xi\mapsto\norm{f(\xi)}\) is regulated, by a remark from~(7.6) above. If \(g\)~is a primitive of~\(f\) and \(h\)~is a primitive of \(\xi\mapsto\norm{f(\xi)}\) (8.7.2), then for all~\(\xi\) not in some denumerable set~\(D\), \(\norm{g'(\xi)}\le h'(\xi)\), hence
\[\norm{g(\beta)-g(\alpha)}\le h(\beta)-h(\alpha)\le(\beta-\alpha)\sup_{\xi\in I-D} h'(\xi)\]
by (8.5.1) and~(8.5.2), as desired.
\end{rmk}

\begin{rmk}
If \(f:[\alpha,\beta]\to\R\) is regulated and \(m\le f\le M\), then
\[m(\beta-\alpha)\le\int_{\alpha}^\beta f\le M(\beta-\alpha)\]
by the mean value theorem~(8.5.3).
\end{rmk}

\subsection*{Section~8}
\begin{rmk}
The function~\(\inv{x}\) is regulated on~\([1,a]\) by (4.1.4) and~(7.6.2), and
\[\int_1^a\inv{x}\dx=\log a-\log 1=\log a\]
since \(\log x\)~is a primitive of~\(\inv{x}\). This integral can serve as a definition of~\(\log a\).
\end{rmk}

\subsection*{Section~9}
\begin{rmk}
In the remarks above~(8.9.1), the partial mappings \(x_1\mapsto f(x_1,a_2)\) and \(x_2\mapsto f(a_1,x_2)\) are continuous on open subsets of \(E_1\) and~\(E_2\), respectively, by (3.20.12) and~(3.20.14), so it makes sense to talk about their differentiability.
\end{rmk}

\begin{rmk}
In the proof of~(8.9.1), we see that \emph{differentiability} alone implies partial differentiability (but the converse is not true!) and (8.9.1.1)~holds as long as \(f\)~is differentiable at~\((x_1,x_2)\); it implies
\[Df(x_1,x_2)=D_1f(x_1,x_2)\after p_1+D_2f(x_1,x_2)\after p_2\]
which implies\footnote{The equation in the book is incorrect.}
\[Df=P_1\after D_1f+P_2\after D_2f\]
where \(P_k(\varphi)=\varphi\after p_k\). Now \(P_k\in\L(\L(E_k,F),\L(E_1\times E_2,F))\) by (5.7.5) and~(5.5.1), so \(Df\)~is continuous if \(D_1f\) and~\(D_2f\) are. Conversely, \(D_kf=I_k\after Df\), where \(I_k(\psi)=\psi\after i_k\), so \(D_kf\)~is continuous if \(Df\)~is.

In the proof that continuous partial differentiability implies (continuous) differentiability, it is not sufficient to appeal to the existence of~\(D_2f(a_1+t_1,a_2)\) to establish the third displayed inequality, because then \(t_2\)~would depend on~\(t_1\); it is necessary to appeal to the continuity of~\(D_2f\) to ensure the inequality holds for all sufficiently small \(t_1\)~and~\(t_2\).
\end{rmk}

\begin{rmk}
In~(8.9.2), let \(g=(g_i)\) and \(h=f\after g\), so \(g\)~is differentiable by~(8.1.5) with \(Dg(x)=(Dg_i(x))\) and \(h\)~is differentiable by the chain rule~(8.2.1) with \(Dh(x)=Df(g(x))\after Dg(x)\). It follows from~(8.9.1) that\footnote{The equation in the book is incorrect.}
\[Dh(x)=\sum_{i=1}^nD_if(g(x))\after Dg_i(x)\]
This holds as long as \(f\)~and the~\(g_i\) are \emph{differentiable} (by the previous remark), but if in addition \(Df\)~and the~\(Dg_i\) are continuous, then \(Dh\)~is continuous (by reasoning as in the previous remark).
\end{rmk}

\subsection*{Section~10}
\begin{rmk}
In~(A), if \(F\)~is \(\R\) or~\(\C\), then \(D_kf(\alpha_1,\ldots,\alpha_n)\)~is identified to a \emph{scalar}. For \(\grad f(\alpha_1,\ldots,\alpha_n)=(D_kf(\alpha_1,\ldots,\alpha_n))\in E\),
\[f'(\alpha_1,\ldots,\alpha_n)(x)=\grad f(\alpha_1,\ldots,\alpha_n)\cdot\conj{x}\]
where the product on the right is the dot product in~\(E\). The vector \(\grad f(\alpha_1,\ldots,\alpha_n)\) is the \emph{gradient} of~\(f\) at~\((\alpha_1,\ldots,\alpha_n)\) and has important geometric properties.
\end{rmk}

\begin{rmk}
In~(8.10.1), let \(K\)~be \(\R\) or~\(\C\), \(\varphi=(\varphi_i):K^n\to K^m\), \(\psi=(\psi_i):K^m\to K^p\), and \(\theta=(\theta_i)=\psi\after\varphi:K^n\to K^p\). Then by the chain rule~(8.2.1), \(\theta\)~is differentiable and \(D\theta(x)=D\psi(\varphi(x))\after D\varphi(x)\). It follows from the remarks above~(8.10.1) that\footnote{The equation in the book is incorrect.}
\[(D_k\theta_i(x))=(D_j\psi_i(\varphi(x)))(D_k\varphi_j(x))\]
and similarly for the determinants when \(m=n=p\). This holds as long as the functions \(\varphi_i\) and~\(\psi_i\) are \emph{differentiable}, by a remark from~(8.9) above.
\end{rmk}

\subsection*{Section~12}
\begin{rmk}
In~(8.12.2), note \(D^2f(x_0)\at(s,t)\) and \(D^2f(x_0)\at(t,s)\) both approximate the change in the change in~\(f\) across opposing sides of the parallelogram in~\(A\) with vertices \(x_0\), \(x_0+s\), \(x_0+t\), and \(x_0+s+t\):
\begin{align*}
D^2f(x_0)\at(s,t)=(D^2f(x_0)\at s)\at t&\approx Df(x_0+s)\at t-Df(x_0)\at t\\
	&\approx[f(x_0+s+t)-f(x_0+s)]-[f(x_0+t)-f(x_0)]\\
	&=[f(x_0+t+s)-f(x_0+t)]-[f(x_0+s)-f(x_0)]\\
	&\approx Df(x_0+t)\at s-Df(x_0)\at s\\
	&\approx(D^2f(x_0)\at t)\at s=D^2f(x_0)\at(t,s)
\end{align*}
It is therefore plausible that \(D^2f(x_0)\at(s,t)=D^2f(x_0)\at(t,s)\), in other words that \(D^2f(x_0)\)~is symmetric. The proof uses the mean value theorem (8.6.2) to bound the distances between approximately equal quantities above, and ultimately to establish symmetry.

In the proof, let \(\beta=2\epsilon\norm{s}(\norm{s}+\norm{t})\) and observe
\begin{align*}
\norm{g(1)-g(0)-(f''(x_0)\at t)\at s}&\le\norm{g(1)-g(0)-g'(0)}+\norm{g'(0)-(f''(x_0)\at t)\at s}\\
	&\le\sup_{0\le\xi\le 1}\norm{g'(\xi)-g'(0)}+\beta\\
	&\le\sup_{0\le\xi\le 1}\norm{g'(\xi)-(f''(x_0)\at t)\at s}\\
	&\qquad+\norm{g'(0)-(f''(x_0)\at t)\at s}+\beta\\
	&\le3\beta
\end{align*}
\end{rmk}

\begin{rmk}
If \(E=E_1\times\cdots\times E_n\) and~\(F\) are Banach spaces and \(f:E\to F\) is twice differentiable at \(x\in E\), then each \(D_i f\)~is differentiable at~\(x\) and
\[D^2f(x)\at(s,t)=\sum_{i,j}D_iD_jf(x)\at(s_i,t_j)\]
where \(s=(s_i)\), \(t=(t_i)\), and the sum is taken over all pairs~\((i,j)\) with \(1\le i,j\le n\).
\end{rmk}
\begin{proof}
By a remark from~(8.9) above, \(D_if=I_i\after Df\), so \(D_if\)~is differentiable at~\(x\) by the chain rule (8.2.1). Also \(Df=\sum P_i\after D_if\), so \(D^2f(x)=\sum P_i\after DD_if(x)\) by additivity (8.2.2) and the chain rule, and
\begin{align*}
D^2f(x)\at(s,t)=(D^2f(x)\at s)\at t&=\sum_{j=1}^n(DD_jf(x)\at s)\at t_j\\
	&=\sum_{j=1}^n\left(\sum_{i=1}^nD_iD_jf(x)\at s_i\right)\at t_j\\
	&=\sum_{i,j}D_iD_jf(x)\at(s_i,t_j)\qedhere
\end{align*}
\end{proof}

\begin{rmk}
In the previous remark, if \(E\)~is~\(\R^n\) (resp.~\(\C^n\)), then each~\(D_iD_jf(x)\) can be identified to a vector in~\(F\) (8.10) and
\[D^2f(x)\at(s,t)=\sum_{i,j}D_iD_jf(x)\xi_i\eta_j\]
where \(s=(\xi_i)\) and \(t=(\eta_i)\). Since \(D^2f(x)\)~is a bilinear mapping, it is uniquely determined by its values \(D^2f(x)\at(e_i,e_j)=D_iD_jf(x)\) on pairs of standard basis vectors, and since \(D^2f(x)\)~is symmetric (8.12.1), it follows that (8.12.2)
\[D_iD_jf(x)=D^2f(x)\at(e_i,e_j)=D^2f(x)\at(e_j,e_i)=D_jD_if(x)\]
If \(F\)~is~\(\R\) (resp.~\(\C\)), these values can be identified to scalars in the \emph{hessian matrix} \(Hf(x)=(D_iD_jf(x))\), which is symmetric. If \(s\)~and~\(t\) are identified to column vectors, we have \(D^2f(x)\at(s,t)=s^T Hf(x) t\).
\end{rmk}

\begin{rmk}
In the proof of~(8.12.4), for \(g(x)=D^{p-2}f(x)\at(t_3,\ldots,t_p)\),
\[Dg(x)\at t_2=D^{p-1}f(x)\at(t_2,\ldots,t_p)\]
by the remark above~(8.12.4), and hence
\[D^2g(x)\at(t_1,t_2)=D^pf(x)\at(t_1,\ldots,t_p)\]
by (8.12.1) and the remark above~(8.12.4) again.
\end{rmk}

\begin{rmk}
In the proof of~(8.12.5), if \(D^mf\)~is \((n+1)\)-times differentiable, then \(D^mf\)~is \(n\)-times differentiable and \(D^n(D^m f)\)~is differentiable, by definition. By induction, \(f\)~is \((m+n)\)-times differentiable and \(D^{m+n}f=D^n(D^mf)\). Therefore \(f\)~is \((m+n+1)\)-times differentiable and
\[D^{m+n+1}f=D(D^{m+n}f)=D(D^n(D^mf))=D^{n+1}(D^mf)\]
\end{rmk}

\begin{rmk}
The converse of~(8.12.5) holds, by induction on~\(n\). The result yields an exponentiation law for the differential operator:
\[D^m D^n=D^{m+n}=D^n D^m\]
\end{rmk}

\begin{rmk}
If \(E=E_1\times\cdots\times E_n\) and~\(F\) are Banach spaces and \(f:E\to F\) is \(p\)-times differentiable at \(x\in E\), then
\[D^pf(x)\at(t_1,\ldots,t_p)=\sum_{j_1,\ldots,j_p}D_{j_1}\cdots D_{j_p}f(x)\at(t_{1,j_1},\ldots,t_{p,j_p})\tag{1}\]
where \(t_i=(t_{i,j})\) for \(1\le i\le p\) and \(1\le j\le n\), and the sum is taken over all \(n^p\) \(p\)-tuples \((j_1,\ldots,j_p)\) with \(1\le j_i\le n\) for \(1\le i\le p\).
\end{rmk}
\begin{proof}
By induction on~\(p\). For \(p=1\), the result holds by~(8.9). If the result holds for~\(p\) and \(f\)~is \((p+1)\)-times differentiable at~\(x\), then each partial \(D_{j_1}\cdots D_{j_p}f\) is differentiable at~\(x\) since
\[D_{j_1}\cdots D_{j_p}f=I_{j_1,\ldots,j_p}\after D^pf\]
where \(I_{j_1,\ldots,j_p}(\psi)=\psi\after(i_{j_1}\times\cdots\times i_{j_p})\). Here \(i_{j_k}:E_{j_k}\to E\) is just the canonical injection, and \(i_{j_1}\times\cdots\times i_{j_p}:E_{j_1}\times\cdots\times E_{j_p}\to E^p\) maps \((x_k)\) to~\((i_{j_k}(x_k))\). \(I_{j_1,\ldots,j_p}\)~is a continuous linear mapping from~\(\L_p(E,F)\) to the space of continuous \(p\)-linear mappings from \(E_{j_1}\times\cdots\times E_{j_p}\) to~\(F\). Differentiating both sides of~(1) with respect to~\(x\) (keeping the~\(t_i\) fixed) yields the result for~\(p+1\).
\end{proof}

\begin{rmk}
If \(E\)~is \(\R^n\) or~\(\C^n\), the previous remark yields~(8.12.7). In this case each \(D_{j_1}\cdots D_{j_p}f(x)\) can be identified to a vector in~\(F\) and it follows from~(8.12.4) that
\begin{align*}
D_{j_1}\cdots D_{j_p}f(x)&=D^pf(x)\at(e_{j_1},\ldots,e_{j_p})\\
	&=D^pf(x)\at(e_{j_{\sigma(1)}},\ldots,e_{j_{\sigma(p)}})=D_{j_{\sigma(1)}}\cdots D_{j_{\sigma(p)}}f(x)
\end{align*}
for any permutation~\(\sigma\) of \(1,\ldots,p\).
\end{rmk}

\begin{rmk}
In the remark above,
\[D^pf=\sum P_{j_1,\ldots,j_p}\after D_{j_1}\cdots D_{j_p}f\]
where \(P_{j_1,\ldots,j_p}(\varphi)=\varphi\after(p_{j_1}\times\cdots\times p_{j_p})\). It follows from this and the converse relation above, with~(8.9.1), that \(D^pf\)~exists and is continuous if and only if each \(D_{j_1}\cdots D_{j_p}f\) exists and is continuous. If \(E\)~is \(\R^n\) or~\(\C^n\), this yields~(8.12.8).
\end{rmk}

\begin{rmk}
In the proof of~(8.12.10), the key observation is that
\[h':x\mapsto ((g'\after f)(x),f'(x))\mapsto (g'\after f)(x)\after f'(x)\]
\end{rmk}

\subsection*{Section~13}
\begin{rmk}
In the proof of~(8.13.1), we only have the \emph{real} exponential function from~(8.8), although the same proof does work with the complex exponential function by~(9.5.3). Note \(D_if(x)=\lambda_if(x)\), hence \(D^{\alpha}f(x)=M_{\alpha}(\lambda_1,\ldots,\lambda_n)f(x)\).
\end{rmk}

\begin{rmk}
In~(8.13.2), taking \(P(X)=X^p\) and \(A=E=F=G=\R\) (resp. \(=\C\)) yields
\[(fg)^{(p)}=\sum_{k=0}^p\binom{p}{k}f^{(p-k)}g^{(k)}\]
by the binomial theorem.
\end{rmk}

\subsection*{Section~14}
\begin{rmk}
In Taylor's theorem~(8.14.2), let
\[P(\alpha)=\sum_{k=0}^{p-1}\frac{(\xi-\alpha)^k}{k!}f^{(k)}(\alpha)\qquad\text{and}\qquad R(\alpha)=\int_{\alpha}^{\xi}\frac{(\xi-\zeta)^{p-1}}{(p-1)!}f^{(p)}(\zeta)\dzeta\]
Then \(P(\alpha)\)~is the \((p-1)\)-th Taylor polynomial for~\(f\) about~\(\alpha\) evaluated at~\(\xi\), and \(R(\alpha)\)~is the corresponding integral remainder, so \(f(\xi)=P(\alpha)+R(\alpha)\).

In the proof, for \(g(\zeta)=(\xi-\zeta)^{p-1}/(p-1)!\),
\[g^{(k)}(\zeta)=(-1)^k\frac{(\xi-\zeta)^{p-1-k}}{(p-1-k)!}\qquad(0\le k\le p-1)\]
and \(g^{(p)}(\zeta)=0\). Substituting \(p-1-k\) for~\(k\) yields
\[\frac{(\xi-\zeta)^k}{k!}f^{(k)}(\zeta)=(-1)^{p-1-k}g^{(p-1-k)}(\zeta)f^{(k)}(\zeta)\qquad(0\le k\le p-1)\]
It follows from~(8.14.1) that \(DP(\zeta)=g(\zeta)f^{(p)}(\zeta)\), so
\[f(\xi)-P(\alpha)=P(\xi)-P(\alpha)=\int_{\alpha}^{\xi}g(\zeta)f^{(p)}(\zeta)\dzeta=R(\alpha)\]
It is more natural to run this proof in reverse, first defining the remainder \(S(\alpha)=f(\xi)-P(\alpha)\), then computing \(DS(\zeta)=g(\zeta)f^{(p)}(\zeta)\), so \(S(\alpha)=S(\alpha)-S(\xi)=R(\alpha)\).
\end{rmk}

\begin{rmk}
In the proof of Taylor's theorem~(8.14.3), \(\int_0^1(1-\zeta)^{p-1}/(p-1)!\dzeta=1/p!\), so the mean value theorem~(8.7.7) yields
\begin{align*}
&\quad\bignorm{\int_0^1\frac{(1-\zeta)^{p-1}}{(p-1)!}f^{(p)}(x+\zeta t)\dzeta-\frac{1}{p!}f^{(p)}(x)}\\
&=\bignorm{\int_0^1\frac{(1-\zeta)^{p-1}}{(p-1)!}\left[f^{(p)}(x+\zeta t)-f^{(p)}(x)\right]\dzeta}\\
&\le\int_0^1\frac{(1-\zeta)^{p-1}}{(p-1)!}\bignorm{f^{(p)}(x+\zeta t)-f^{(p)}(x)}\dzeta\\
&\le p!\,\epsilon\int_0^1\frac{(1-\zeta)^{p-1}}{(p-1)!}\dzeta=\epsilon
\end{align*}
\end{rmk}

% References
\begin{thebibliography}{0}
\bibitem{dieudonne} Dieudonn\'e, J. \textit{Foundations of Modern Analysis.} Academic Press, 1960.
\end{thebibliography}
\end{document}
